\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {paragraph}{This is my course note on “Trees And Graphs: The Basics” provided by Colorado University of Boulder. This is a non-credit prep course for an MS-CS degree.}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Binary Search Trees}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Basic Concepts}{3}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Binary search tree is a binary tree is a kind of data type with set of data elements without repeatition.\\ We can insert, delete, search, and traverse the data elements in a binary search tree.\\ For each element in it, there will be a key of the element, which will always be a number.\\ With this setting in place, we can always comparing different elements by comparing their keys, even if the elements are not numbers.\\ }{3}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ In the figure, we have a binary search tree with some nodes and leaves. Every node has two children nodes and those leaves, which have no children nodes, are called nil nodes.\\ Every node has an element with a key, and the key of the left child node is always $<$ the key of the parent node, and the key of the right child node is always $>$ the key of the parent node.\\ The left and right child nodes are also binary search trees.\\ That is to say, the keys are always in a sorted order regardless of the structure of the tree. When we move the elements around, the keys will be different for each elements, in order to remain in the sorted order.\\ The leaves have no elements.\\ }{4}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ When there is a node with the key 25, every node in the left subtree will have a key $<$ 25, and every node in the right subtree will have a key $>$ 25.\\ The rule will also apply to all those subtrees.\\ }{4}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}The Height of a Binary Search Tree}{5}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ The height of a binary search tree is the number of edges on the longest path from the root node to a leaf node.\\ We will define the height of a leaf node as 0. Then the height of number 25, a.k.a. the root node of the below binary search tree is 2.\\ }{5}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Let's assume we have a balanced binary search tree with n internal nodes.\\ One each layer from the root node, there'll be $2^0$ nodes, $2^1$ nodes, $2^2$ nodes, $\cdots  $, $2^h$ nodes.\\ The total number of nodes in the tree will be $2^0+2^1+2^2+ \cdots  +2^h=2^{h+1}-1$.\\ So, the height of the tree will be $h=\log  _2(n+1)-1$.\\ In the sense of big O notation, the height of a binary search tree is $O(\log  _2(n))$, in a balanced binary search tree scenario.\\ For example, $\log  _2^8=3$, so the height of a binary search tree with 8 nodes is 3.\\ $\log  _2^{15}=4$, so the height of a binary search tree with 15 nodes is 4.\\ }{6}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ In the worst case scenario, where the binary search tree is not balanced, the height of the tree will be $O(n)$.\\ That is to say, the tree will be a linked list looks like this;\\ }{6}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ In this case, the height of the tree is 6, which is equal to the number of nodes in the tree.\\ The height of the tree is $O(n)$, which is the worst case scenario.\\ Normally, we will have something in between the best case scenario and the worst case scenario, $O(\log  (n)) < height < O(n)$.\\ }{7}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Basics of Binary Search Tree Quiz}{8}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Insertion and Deletion in a Binary Search Tree}{10}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Insertion}{10}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ We can insert a new element into a binary search tree by comparing the key of the new element with the key of the root node.\\ If the key of the new element is less than the key of the root node, we will insert the new element into the left subtree.\\ If the key of the new element is greater than the key of the root node, we will insert the new element into the right subtree.\\ We will repeat the process until we reach a leaf node.\\ For example, we have a binary search tree with the following nodes;\\ }{10}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ If we want to insert a new element with the key 40, we will compare 40 with 25, and then 40 with 50, and then 40 with 35.\\ Since 40 is greater than 35, we will insert 40 as the right child node of 35.\\ }{11}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ The above example actually consists of two steps.\\ First, we will search for the element to be inserted, which is called the $find()$ operation.\\ Then, after successfully locates the element, we will insert it into the binary search tree.\\ We will talk about find operation now.\\ Assuming we have an impefect binary search tree and we want to locate the key $45$, how should we do that?\\ }{11}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ We will start from the root node, and then compare the key of the root node with the key of the element to be located.\\ If the key of the root node is equal to the key of the element to be located, we will return the root node. If the key of the root node is greater than the key of the element to be located, we will search the left subtree. Otherwise, we will search the right subtree.\\ The overall process will be repeated until we reach a leaf node.\\ If we reach a leaf node and the key of the leaf node is not equal to the key of the element to be located, we will return NIL.\\ }{12}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ As for the time complexity, the find operation will take $O(h)$ time, where $h$ is the height of the binary search tree.\\ Now we will go ahead with the second step of the insertion operation.\\ We will insert the new element into the binary search tree by comparing the key of the new element with the key of the root node.\\ }{12}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.2}Deletion}{13}{subsubsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Now we will talk about the deletion operation.\\ There are three cases to consider when deleting a node from a binary search tree.\\ The node to be deleted can be a leaf node, a node with only one child,or one with two child nodes.\\ If both child nodes are NIL, we can simply delete the node.\\ If one of the child nodes is NIL, we can delete the node, then reconnect between the past-parrent node and past-child node, like this;\\ }{13}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ What if we want to delete a node that has two non-NIL children?\\ We will find the smallest node in the right subtree of the node to be deleted, and then replace the node to be deleted with the smallest node.\\ To perform this operation, we will `walk' right from the root node by one step and then `walk' left until we reach a leaf node.\\ During each step, we will compare the key of the current node with the key of the node to be deleted.\\ The leaf node we reached will be the successor of the node we deleted.\\ }{14}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.3}Tree Traversal}{15}{subsubsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ There are three ways to traverse a binary search tree.\\ In-order traversal, pre-order traversal, and post-order traversal.\\ Inorder traversal visits nodes in a binary tree in the following order:\\ Visit the left subtree.\\ Visit the root node.\\ Visit the right subtree.\\ This traversal method is particularly useful for binary search trees (BSTs) because it visits the nodes in ascending order.\\ }{15}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Preorder traversal visits nodes in the following order:\\ Visit the root node.\\ Visit the left subtree.\\ Visit the right subtree.\\ Preorder traversal is useful for creating a copy of the tree or getting a prefix expression of an expression tree.\\ }{17}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Postorder traversal visits nodes in the following order:\\ Visit the left subtree.\\ Visit the right subtree.\\ Visit the root node.\\ Postorder traversal is useful for deleting a tree or evaluating postfix expressions of an expression tree.\\ }{18}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}BST Quiz}{19}{subsection.1.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Question 1}}{19}{figure.1}\protected@file@percent }
\newlabel{fig:sidebyside}{{1}{19}{Question 1}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Question 2}}{20}{figure.2}\protected@file@percent }
\newlabel{fig:sidebyside}{{2}{20}{Question 2}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Question 3}}{21}{figure.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Question 4}}{22}{figure.4}\protected@file@percent }
\newlabel{fig:sidebyside}{{4}{22}{Question 4}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Algorithms on Trees}{23}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Red-Black Trees Basics}{23}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Normally the performance of a BST depends on its height or depth.\\ If we have a bad (not well-balanced) binary search tree, the height of the tree will be $O(n)$.\\ In this case, the performance of the tree will be the same as that of a linked list.\\ If it's balanced, the running time will be as good as $O(\log  (n))$. This is not good enough.\\ What we need is self-balancing BSTs.\\ Here are some different types of self-balancing BSTs.\\ }{23}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Red-black trees are a type of self-balancing binary search tree.\\ They are named after their property of having red and black nodes.\\ The red-black tree has the following properties:\\ 1. Every node is either red or black.\\ 2. The root node is always black.\\ 3. Every leaf node is black.\\ 4. If a red node has children, the children must be black.\\ 5. Every path from a node to its descendant NIL nodes must have the same number of black nodes.\\ The height of a red-black tree is $O(\log  (n))$.\\ The red-black tree is a balanced binary search tree.\\ }{25}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ When counting the number of black nodes, we do not count the red nodes in between.\\ }{25}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Here is a question regarding red-black trees.\\ We can come up with two scenarios for it.\\ }{26}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ As we can see, the tree in the figure is not a red-black tree.\\ On the left corner, the height of the root node is 2, however, the height of the middle subtree is 3.\\ For any red-black tree with $n$ nodes, the height $h$ must obey the following rule: \\ $\log  (n) \leq h \leq 2*\log  (n)$.\\ Here is another example of a unbalanced tree that cannot satisfy the red-black tree properties.\\ }{28}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Now we can try to figure out how can we find a red-black tree.\\ Suppose we have a $n$ node, $h$ height binary search tree.\\ If it is a red-black tree, any path $l$ in the tree must be $h/2 < l < h$, since on any given path, at least half of the nodes must be black.\\ }{30}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Let's assume the black height of a specific node is $bh$. The subtree under that node will have a total node count $nc \geq 2^{bh} - 1$.\\ This can be proved by induction, or we can also directly see the logic via such graph:\\ }{30}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Since we have the property that any path $l \geq h/2$, we can say that $nc \geq 2^{bh} - 1 \geq 2^{h/2} - 1$.\\ Then we can conclude that $h \leq 2*\log  (n+1)$.\\ }{31}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}RBT Quiz}{32}{subsubsection.2.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces RBT Question 1}}{32}{figure.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces RBT Question 2}}{33}{figure.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces RBT Question 3}}{34}{figure.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Red-Black Tree Rotation, Algorithms for Insertion/Deletion}{35}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ As we know, red-black trees are a type of self-balancing binary search tree.\\ In order to perform a $find()$ operation, what we do is exactly the same as those of general BSTs.\\ However, when we insert or delete a node, we need to maintain the red-black tree properties.\\ To find a particular key in RBT, we will find its height $h$ first.\\ Then we will find the black height $bh$ of the node.\\ Since $\theta (h) = \theta (\log  _2^n)$, therefore the worst case of running there is $\theta (\log  _2^n)$.\\ }{35}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ When performing insert function on RBT, sometimes we encounter a situation where the parent node is red, which is called a red-red violation, due to the rule that any inserted nodes will be marked red.\\ In this case, we need to perform a rotation operation.\\ }{36}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Suppose we have a red-red violation as below.\\ x is the newly inserted node, y is the parent node of x, and z is the parent node of y.\\ What we can do is that we cange the color of y and w to black, and the color of z to red.\\ Since root node cannot be red, then we need to add a new root node for the tree in order to make it valid.\\ }{36}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ In another case, if node w is black, we need to perform a rotation operation.\\ There are two types of rotation operations, left rotation and right rotation.\\ The left rotation operation is as follows.\\ }{37}{section*.31}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Wikipedia Tree Rotation Explanation}}{39}{figure.8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ If we encounter some unique cases, we need to perform a double rotation as follows.\\ }{40}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Firstly, we will do a left rotation on the x and y subtree section.\\ }{40}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Then we will do a right rotation on the z and x subtree section.\\ }{41}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Now we can see that the tree is a valid red-black tree.\\ }{41}{section*.35}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Tree Rotation Quiz}}{41}{figure.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Insertion and deletion both has the complexity of $O(\log  _2^n)$.\\ This is not too much of the problem, the problem is that we need to maintain the red-black tree properties after each insertion and deletion.\\ This is not easy to do, and we need to be careful to ensure that the tree remains balanced and valid after each operation.\\ }{42}{figure.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Skip Lists}{42}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Skip lists are a data structure that provides an alternative to balanced trees for implementing dictionaries.\\ They are similar to balanced trees in that they maintain a sorted list of keys, but they use a different approach to achieve balance.\\ In a skip list, each node has a level, and the level of a node is determined by the number of nodes in the list that are above it.\\ The higher the level of a node, the more nodes it has above it.\\ The levels of the nodes form a staircase pattern, with each level being a power of 2.\\ The height of the skip list is the number of levels in the highest node.\\ }{43}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ For any layer in the skip list, the elements are sorted in ascending order from left to right.\\ The elements in the lower layer are the elements in the higher layer, plus some extra elements in between.\\ The extra elements are the ones that are not in the lower layer. Therefore, all the top layers are just subsets of the bottom layer.\\ }{44}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ How do we use skip lists to implement dictionaries? We can use the following operations:\\ 1. $find()$: Find the node with the given key in the skip list.\\ 2. $insert()$: Insert a new node with the given key into the skip list.\\ 3. $delete()$: Delete the node with the given key from the skip list.\\ How does the $find()$ operation work? We start from the topmost layer of the skip list, and we will compare the key with the current node.\\ If the key is greater than the current node, we will move to the right to the next node in the same layer.\\ If the key is less than the current node, we will go back to the previous node in the same layer, and move down to the next node in the lower layer.\\ We will continue this process until we reach the bottommost layer.\\ If we reach the bottommost layer and still cannot find the key, we will return NIL.\\ }{45}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ What is the average height of a skip list?\\ The average height of a skip list is $O(\log  _2^n)$.\\ This is because the height of a skip list is determined by the number of levels in the highest node, and the number of levels in the highest node is $O(\log  _2^n)$.\\ }{46}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ The probability of a node exceeding height $h$ is $(1/2)^h$.\\ This is because the probability of a node exceeding height $h$ is the probability of a node exceeding height $h-1$, and the probability of a node exceeding height $h-1$ is $1/2^{h-1}$.\\ Therefore, the probability of a node exceeding height $h$ is $(1/2)^h$.\\ }{46}{section*.41}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Skip List Quiz}}{47}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ This is similar to the Boolie's inequality in probability theory.\\ Boolie's inequality is a probability inequality that bounds the probability of the union of events by the sum of the probabilities of the individual events.\\ In this case, the events are the nodes exceeding height $h$, and the probability of a node exceeding height $h$ is $(1/2)^h$.\\ The sum of the probabilities of the events is the expected height of the skip list.\\ Therefore, the expected height of the skip list is $O(\log  _2^n)$.\\ $P(>h) \leq \sum _{i=h}^{\infty } P(i) = \sum _{i=h}^{\infty } (\frac  {1}{2})^i = \frac  {1}{2^h}$.\\ }{47}{figure.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Graphs}{48}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Graphs and Their Representations}{48}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ A graph is a data structure that consists of a set of vertices and a set of edges.\\ The vertices are the nodes of the graph, and the edges are the connections between the nodes.\\ There are two types of graphs: directed graphs and undirected graphs.\\ In a directed graph, the edges have a direction, and in a undirected graph, the edges do not have a direction.\\ Normally we don't consider the self-loop in the graph. There should be no multi-edges between vectices.\\ }{48}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Where do graph come from?\\ Graphs are used to model relationships between objects.\\ For example, a social network can be represented as a graph, where the vertices are the people in the network and the edges are the connections between the people.\\ Graphs are also used to model networks, such as the internet (computer network), where the vertices are the computers in the network and the edges are the connections between the computers.\\ Other networks such as Ecological networks, electrical circuits, and transportation networks can also be represented as graphs.\\ How do we represent a graph in a computer?\\ There are two common ways to represent a graph in a computer: adjacency matrix and adjacency list.\\ The adjacency matrix is a 2D array that stores the connections between the vertices.\\ The row and column indices are actually vertices, when there's an edge, the value will be 1, otherwise 0.\\ }{50}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ For a graph with $k$ nodes, the adjacency matrix will be a $k*k$ matrix.\\ There will be up to $k(k-1)$ edges in the graph.\\ Then we come to the adjacency list.\\ The adjacency list is a list of lists that stores the connections between the vertices.\\ Each vertex has a list of its neighbors.\\ The adjacency list is more space-efficient than the adjacency matrix, especially for sparse graphs.\\ The adjacency list is also more time-efficient for some operations, such as finding the neighbors of a vertex.\\ Here is a example as denoted in the figure.\\ }{51}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Graph Traversals and Breadth First Traversal}{51}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Queue: FIFO(First In First Out)}}{51}{figure.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ There are two common ways to traverse a graph: breadth-first traversal and depth-first traversal.\\ In a breadth-first traversal, we start at a vertex and visit all of its neighbors before moving on to the next level of neighbors.\\ In a depth-first traversal, we start at a vertex and visit one of its neighbors, then visit one of the neighbor's neighbors, and so on, until we reach a dead end.\\ Then we backtrack and visit another neighbor of the original vertex.\\ Breadth-first traversal/Search is useful for finding the shortest path between two vertices in an unweighted graph.\\ Depth-first traversal is useful for finding cycles in a graph.\\ Here is an example of a breadth-first traversal of a graph.\\ }{52}{figure.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Here is an indepth explanation of BFS (Breadth-first Search/Traversal). https://blog.csdn.net/g11d111/article/details/76169861. In a simple way, BFS is a traversal algorithm that goes through the graph in a wide range before going deep into the graph further(Neighbors First).\\ On the other side, the depth-first traversal goes into the graph deeply from one neighbor of the starting point before going through another neighbor.\\ }{52}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Depth First Search}{53}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Stack: LIFO(Last In First Out) The stack is used to store the nodes that we have visited but have not yet explored all of their neighbors.\\ When we visit a node, we push it onto the stack.\\ When we finish exploring all of the neighbors of a node, we pop it off the stack.\\ }}{53}{figure.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ Depth-first search is a graph traversal algorithm that starts at a vertex and explores as far as possible along each branch before backtracking.\\ It is similar to the way we would explore a maze, where we would go down one path until we reach a dead end, then backtrack and try another path.\\ Depth-first search is useful for finding cycles in a graph.\\ Here is an example of a depth-first search of a graph.\\ }{54}{figure.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ When we go through DFS, we will visit all the successors from one neighbor of the root node first.\\ Then we will go back to the root node.\\ While doing this, if any of the visited nodes have other successors that have not been visited, we will visit them first.\\ This is the main difference between BFS and DFS.\\ }{54}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ What is a DFS Tree?\\ A DFS tree is a tree that is created by a depth-first search of a graph.\\ It is a subgraph of the original graph that contains all the vertices and edges that are visited during the depth-first search.\\ It is directed, where each edge points from a parent vertex to a child vertex.\\ And what is a back edge?\\ A back edge is an edge that connects a vertex to one of its ancestors in the DFS tree. It may connect back to their grandparent, grand-grandparent and so on.\\ It is a cycle in the graph.\\ If we encounter a back edge during a depth-first search, we know that the graph contains a cycle.\\ }{55}{section*.50}\protected@file@percent }
\gdef \@abspage@last{55}
