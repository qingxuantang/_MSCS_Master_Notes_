\relax 
\@writefile{toc}{\contentsline {paragraph}{This is my course note on “Algorithms for Sorting, Searching and Indexing” provided by Colorado University of Boulder. This is a non-credit prep course for an MS-CS degree.}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Asymptotic Notation: Big O}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Time and Space Complexity}{5}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We have some questions regarding how to figure out which algo is faster.\\ Q1: what input should we choose.\\ Q2: how to evaluate time.\\ When considering implementation details, that is under the realm of performance analysis. We don't want to be bogged down to this level and only wish to focus on the algorithm.\\ The runtime is the number of basic operations. For each sorting algorithm, there will be a range of possible runtime, due to the differences between inputs, the best case, average case, and worst case respectfully.\\ The average time cost is hard to analyze while may be more desirable than the worst case cost, which is too pessimistic in some cases. Never use best case cost.\\ In most of cases we will use worst case cost to evaluate.\\ Let's assume the cost of Insertion Sort is $f(n)$.\\ Hypothetically (just for examples) speaking:\\ $ f(n) = 0.05n^2 + 1.5n +70 $\\ We assume that the different algorithms such as addition or multiplication have the same unit cost, that is to say, they cost the same under one unit run. But is that the real case?\\ Of course not. Different operations have different costs for a unit run. To change that, we can change the coefficient of the original equations.\\ We care about which algorithm is faster asymptotically. This is why this kind of analysis is called asymptotic analysis.\\ The naming is inspired by one idea, that is the comparison should be within the realm of actual usage, for instance, when sorting 10 numbers, algo1 may be slower than algo2, but while sorting 10000 numbers, algo1 may outfast the opposite, that's why we compare them asymptotically.\\ And to be aware, we focus more on cases with large numbers or unit cases than small samples.}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Big O}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{$ f = O(g(n)) $\\ Meaning: $f()$ is ASUMPTOTICALLY upper bounded by $g(n)$\\ Here are some properties:\\ When $g()$ overtakes $f()$, meaning in the chart, g is above f after some time, and then f will remain overtaken forever.\\ The constant factor must be disregarded.\\}{6}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{$ \exists k>0,N_0, \forall n \geq N_0, f(n) \leq k*g(n) $\\ $f(n) \leq k*g(n)$ only happens beyond some point in time. When the above condition happens, we say:\\ $ f(n) = O(g) $\\ Let's take some examples:\\ $ f=(1/2)n^2 $\\ $ g=0.1n^3 $\\ Assuming k=10, and $N_0$=1\\ Using the equation above $f(n) \leq k*g(n)$, then we will have $f(n) \leq g(n)$, which has got rid of the constant k;\\ Then we can say,\\ $f(n) = O(g) $\\ a.k.a. \\ $0.5n^2=O(0.1n^3) $\\ As long as $g(n)$ will overtake $f(n)$ after K, $g(n)$ will be the cost of $f(n)$, aka $f(n)=O(g)$.\\ Under such circumstances, adding some constants to the equation of f and g, won't change the fact that $f(n)=O(g)$.\\ The $g(n)$ must always be less than or equal to $f(n)$ to make $f(n) = O(g)$ valid: that is to say, after a certain point $N_0$, the output of the function $f(n)$ is less than or equal to the output of the function $g(n)$.}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Big Omega, Big Theta, and Examples}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}$f(n)=\Omega (g)$}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{f is asymptotically lower bounded by g (n): that is to say, after a certain point $N_0$, the output of the function f (n) is bigger or equal to the output of the function g (n).Since O () and $\Omega ()$ are the opposite, if f (n) = O (g), then $g(n)=\Omega (f)$.}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}$f(n)=\Theta (g)$}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{f is asymptotically equal to g (n). This looks like a Bollinger band in technical analysis in some ways.\\ $\exists k_1,k_2, \forall n \geq N_0,k_1(g) \geq g \geq k_2(g)$\\ If $f = \Theta (g)$, it means f is asymptotically equal to $\Theta (g)$, which we can say,\\ $f = O(g), f = \Omega (g)$\\ When dealing with functions, we will typically get rid of constant figures in the function, however,\\ we cannot ignore the exponent constant such as the 2 and 4 in 2 to the power of 4x. \\ $2^{4x}$\\ Here is an example of big theta.\\ $f(n)=2n+3\log  _2^n+5$\\ $g(n)=15n+35\log  _2^n+\sqrt  {n}+17$\\ In the above equations, n is the only thing that matters. Therefore we can ignore the log and square root elements by considering them as constants.}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Examples of Asymptotic Notations}{8}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Here are five functions;\\ $f(n)=2n^2 + 3\log  _2^n + 4\sqrt  {n} + 15$\\ $g(n)=200\sqrt  {n} + 15\log  _2^n + 14n\sqrt  {n}$\\ $h(n)=n^2 + 2n + 3\log  ^{logn}$\\ $l(n)=n^3 + 15n^2 + 2.5n$\\ $m(n)=4n^2 + 13n^2\log  _2^n$\\ Let's find out which element with n is the BIGGER one.\\ $n^2=n*n > nlogn > \sqrt  {n}$\\ Before we can compare these functions, we should determine the dominating term of each one:\\ $f: n^2$\\ $g: n^{1.5}$\\ $h: n^2$\\ $l: n^3$\\ $m: n^2logn$\\ In this case, we can place these functions in such order, $l > m > f > h > g$\\ The smaller one is the Omega of the bigger one, and vice versa. For example, $l(n)=\Omega (m)$, $m=O(l)$, and so on.}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Binary Search}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}what do we need to do in a binary search?}{10}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We need to check if a given element, say 6, is located in the given list. The prerequisite is that the list must be sorted to do a binary search.\\ Let's assume the list is sorted in ascending order. First, we need to find the middle element of the list by calculating the index number of left and right border elements, a.k.a. the first and last one in the list.\\ Here is how we do it:\\ $mid=(left + right) // 2$\\ In the equation, // means floor divided, which will return exact the middle element with odd number list and the smaller one of the middle two elements in even number list.\\ Now we will check if the middle one equals to the given element. If so, problem solved! If not, and the given one is smaller than the middle, we will focus on the left side of the list, a.k.a. the elements located on the left of the middle one. In this case, if the first middle has index n, then the new “right” border element will have the index n-1.\\ If the middle element is smaller, of course we will focus on the right portion of the original list since this is an ascendibg ordered list. And in this case, if the first middle element's index is n, the new “left” will have the index n+1. Then we will conduct the calculation by finding the new middle on the left/right part of the list (a.k.a. the new range with the updated left or right border). We will compare the given value with the new middle.\\ We will repeat the process untill either find the element in the list, returning True, or not, which is a little bit complecated:\\ By the end of the process, the updated left border index will be bigger than the right border index in an ascending ordered list, which is not possible to find a valid range. This means the given element is not in the list.\\ For example, there is a list [1,4,9,15,20], and the given element is 8.\\ First we will find out the mid one by calculating the indexes:\\ $(0+4) // 2 = 2$\\ The value of index 2 is 9, which is bigger than the given value 8. Then we will focus on the left part.\\ The updated right border will be index 1, therefore the new mid will be:\\ $(0 + 1) // 2 = 0$\\ Now the equation gives us element value 1. This is smaller than 8. The current right border index is 1, and the updated left border is 1 as well. So the new range will gives us;\\ $(1 + 1) // 2 = 1$\\ a.k.a. the value of index 1 is 4 which is smaller than 8.\\ And now we enter a crucial moment. The current right border index is 1, but since $4 > 8$, the updated left border index will be changed from 1 to 2.\\ Left index=2, right index =1, which is not valid.\\ Therefore, the given element 8 is not located in the given list. Problem solved.}{11}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Define a Func binarySearchHelper()}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Here is the code implemented of the fuc binarySearchHelper().\\ This function will work as a recursion since it will continuously call a new binarySearchHelper() inside the current one until finished.\\ How to prove correctness?\\ The main property of binary search: if we can find the element in the list, it must be in the searching range.}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Running Time Analysis of Binary Search Algo}{12}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{For the running time analysis, we first assume this list has the length of:\\ $n = 2^k$\\ Each time when we check the middle element, we halve the list in half, then the updated list will have a new lengh of\\ $n/2 = 2^{k-1}$\\ By the end, the list will have a length of\\ $2^{k-k} = 1$\\ Now we narrow down the checking range to only one element and check it with the given value. If they are not the same, we will try to find a new middle for more time, which will produce an invalid range as we mentioned before.\\ That is to say, we will at most run the program k+1 times until finished. So by the end, the largest running time will be: \\ $\log  _2^n + 1$\\ Binary search is very efficient. If we have a list of 1 million values, we may only need to search for approximately 21 times.}{13}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Merge Sort}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Mergesort: Correctness and Running Time}{14}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{In order to do a mergesort, firstly we will divide the given array into two parts with the middle element as a separator.\\ For example, we have an array \[1,3,-3,4,-1,2\]\\ We will divide it into \[1,3,-2\] and \[4,-1,2\]\\ Then we will further divide these two into four parts. By now, we have some array contain one one item while others contain two. For those with two items, we will do a swap if they are in the wrong order.\\ These kinds of very small divided array are called base cases, since they are somehow the base unit of this division process.\\ Then we will merge back all those sorted base case arrays, all the way to the top, aka one merged array.\\}{15}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Here is a pseudocode of mergesort. Let's get a closer look at how does it work.\\ At the very beginning, the algo will compare left and right element and check if they are in a valid form, that is to say, whether left (index of the value) is smaller than right (index of the value). If not, the range formed by the left and right index is invalid hence algo ended.\\ Second, we will determine whether the array formed by left and right contains only two elements. If $left + 1 = right$, that means these are adjacent indices. If this is the case, we will swap the elements if they have the wrong order.\\ At last, we enter a position in which the range is larger than two elements array. Now we will simply find the mid element by performing a floor division and do merge sort on left side of the array which is ended by mid and right side which begins with mid+1. This procedure will sort this two subarrays. After that, we will MERGE all the subarray in the order of left, mid and right.\\}{16}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The merge procedure is the next big thing. After this step, all the elements of the array will be sorted. \\ First thing in Merge function, we will assign an initial value to two looping variables, i and j, which are the index of each subarray, by assigning left to i and mid+1 to j. This is to make sure the two loops will begin from the first item of each sub array, aka the left array and right array.\\ We will introduce a temp storage variable \texttt  {tem\_store} to maintain the sorted result and then write back to the original array. Although there are in-place algos that will do the work, it is much easier to just include a third array for storage.\\ The following is straight forward. While i and j are still within the range of each subarray, aka $i \leq mid$ and $j \leq right$, we will compare each i and j pair and store the smaller one to the \texttt  {tem\_store}. Then we add one to the index of the subarray in which the value just compared is smaller, that is to say, move on to the next item. In the meantime, the other array remain the same index, waiting to compare in the next round.\\ After all these steps and iterations, we will find a well sorted \texttt  {tem\_store} array. Then we simply write it back to the orginal array and problem solved.}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Running Time of Merge Procedure}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The running time of merge procedure is precisely the length of the two arrays since each element will be added to the temp store once. We assume each step cost 1, then the total time cost will be len(array A+array B).\\ It can be written as $right - left + 1$.\\ Now let's look at it at higher level.\\ Suppose the original array has lenth of n.\\ The first split will be two n/2 arrays, then the split goes on until it reaches the base unit.\\ For each step, the cost will be the array's count mutiply by the base unit, for example, for the 1st level, the cost will be $2*(n/2)$, for the 2nd, it will be $4*(n/4)$ and so on.\\ Now we can try to conclude a general equation for the time cost of merge sort.\\ Let us assume that $n = 2^k$;\\ Since for each level, the number of level is $n, n/2, n/4, \dots  ,1$;\\ We can write it as $2^{k-1},2^{k-2}, \dots  ,2^{k-k}$.\\ For each level, the time cost is the base unit of that level mutiply by that level's amount of subarrays in that level. For example, the base unit of the 1st level after n is n/2. Then the time cost will be $2 * (n/2)$.\\ Therefore, the generalized time cost will be $n * No.(level)$. Now let's find out what's the number of level. \\ We have already seen this above: $2^{k-1},2^{k-2}, \dots  ,2^{k-k}$, corresponding to level 1, level 2, \textellipsis , last level. We can derive the number of level from this array, $1,2, \dots  ,k$.\\ That is to say, the number of level is k. Ergo $n * No.(level) = n * k$.\\ Since $n = 2^k, k = \log  _2^n$.\\ The final equation will be:\\ $Cost = n * \log  _2^n$\\ If we using the Big O quatation to address to this problem, we get\\ $Mergesort = \Theta (n * \log  _2^n)$\\ And we can get rid of the constant 2 and get\\ $Mergesort = O(n * \log  ^n)$.}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Heap, Min Heap and Max Heap}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Basic Understanding of Heap}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Heap is a type of array that has certain properties. There are the concepts of min heap and max heap and we will go through these soon.\\ First here's a concept called left/right child. Let's say we have a heap of 9 elements. For element number 1, the element number 2 will be called the left child of element 1, and element 3 will be called the right child.\\ While for element 2, actually element 4 will be the left child and 5 the right child.\\ Therefore we have the equation of it:\\ $\forall i,leftchild = 2i,rightchild = 2i + 1$\\ For certain element in the array, If the left/right child element is not inside the current array, then the original one doesn't have left/right child.\\ For example, element 6 has not such child since element 12 and 13 are not in the array of size 9.\\}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We can say i element is the parent of 2i and 2i + 1.\\ for element j, j/2 is the parent. If the division gives a fraction of number then we just round it down (floor division).\\ Element 1 has not parent, and we call it the root.\\}{20}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{If we write the heap array in a parent-child structure, it actually looks like a tree. Yes, heap is also a special binary tree (laid out as an array).\\ Here are some properties:\\ 1. If a node in a heap has a right child, it must have a left child.\\ 2. Heap is an array of size n. $\forall A[i],2i \leq n,A[2i] = leftchild$\\ 3. $\forall A[i],2i+1 \leq n,A[2i+1] = rightchild$\\ And here is the Min Heap Property:\\ The value of the parent node must be less than or equal to the value of children nodes.\\ Min Heap → $ParrentValue \leq ChildrenValues$\\}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{If we turn to the opposite side, we get the Max Heap Property: The value of the parent node must be larger than or equal to the value of children nodes.\\ Max Heap → $ParentValue \geq ChildrenValues$\\ So for any array, we can simply check whether they meet min heap property or max heap property or not in order to determine their identity.\\ Why do we care about min heap or max heap? Here is a basic idea;\\ For any min heap A, we can say A[1], which is the root, is the smallest element of the entire array.\\ We can solve this problem with induction.\\ The Base Case: The root element is the smallest one for Heap depth = 1.\\ And we have the induction hypothesis: Let the result from base case hold for all $depth \leq d$, and then prove it for depth of d + 1.\\ Illustrated as a binary tree, we can see each triangle (consisting of three nodes) as a “sub-heap”. The node in the upper level of the sub-heap is the root of it. For every level from 1 to d, the upper node is always the root of each sub-heap. Therefore the root of level 1 is the smallest in all level from 1 to d.\\ Then we can say that's the same case for level d+1.\\ That is to say, for any min heap A, we can say A[1], the root of it is the smallest element of the entire heap.}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Heap Primatives: Bubble up}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We are interested in the following operations on a heap:\\ 1. Inserting elements into a heap\\ 2. Deleting one from a heap\\ The main primitives of heap are called bubble up and bubble down. We will explain them later.\\ For now, lets assume there is a heap with only one point of failure. Here is an example of such min heap.\\ In the above array, the 6th element, which value is also 6, is smaller than its parrent element 3, with a value of 7. Besides that, the entire array fit the definition of min heap.\\ We can say in this case, the 6th element is in a wrong relation with its parent. It should be moved up in order to make this array a valid min heap.\\ The operation that will acheive this is called bubble up.\\ Bubble Up: the swap of a value with its parrent value if they are in the wrong relation.\\ After we sway the misplaced value with its parent, now we have a new parrent of value 6 and its left child 7. Since the parrent is also less than or equal to its right child. The array is now a valid min heap.\\ Here is another example. We need to bubble up 3 times in order to make it a min heap.\\ \[2,3,7,4,5,9,11,1,8\]\\}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{In the above array, every elements except the last but one are in the right position. Element 8 with value of 1, is smaller than its parrent.\\ Then we need to bubble up value 1 three times into position 4, 2, and 1 to make it work. Now value 1 become the root.\\ We can write a pseudocode for bubble up:\\}{23}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{This is a recursive program that will bubble up the misplaced element until it is in the right place.If we try to estimate the running time of bubble up, the worse case would be O (depth of heap), since the element will be a leaf (the one without child) and it lays on the bottom level of the heap.\\ Then how about $\theta $? Let's assume a heap has n element. If the leaf needed to be bubble up all the way to root position, the steps will be:\\ $n,n/2,n/4, \dots  ,1$\\ Then the steps will be $\log  _2^n$, a.k.a. $\theta (\log  _2^n)$.\\ If we have a heap of $n = 10^6$, which is $2^{19}$, then the total operation required for bubble up is 19.}{24}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Bubble Down}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{In opposite to bubble up, bubble down is the operation that will move the root element to the right position. That is to say, if the root element is not the smallest one, we will swap it with the smallest child. So in the example above, the element 7, which is the parrent of 4 and 3, will be swapped with 3.\\ What if we cannot bubble down once and place the root element in the right position? Sometimes we need to do it multiple times.\\ Here is a min heap that needs to be bubbled down twice.\\}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Now let's take a look at the pseudocode of bubble down.\\}{26}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We will first compare 2j, which is the left child of j, with n, the length of A. If $2j > n $, j has no children whatsoever. \\ Then we go into the second condition, parrent j has only one child, the left one of course. If the parrent is bigger than its child, in this case $A[j] > A[2j]$, we need to swap them.\\ At this moment, we will need to run the recursive procedure, that is to call bubble down again.\\ The third condition is that the parrent has both children. We will compare the left child with the right child. If the left child is smaller, we will compare it with the parrent. If the parrent is bigger, we will swap them. Of course, we will need to recursively run bubble down again.\\ By the end, if the right child is smaller, we will compare it with the parrent. If the parrent is bigger, we will swap them and call the main function recursively.\\ Problem solved.\\}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{How do we estimate the running time of bubble down?\\ The worst case will be the leaf node, which is the last level of the heap. That is to say, we need to bubble down the root element all the way to the bottom of the heap.\\ The time cost will be the depth of the heap, which is $\theta (\log  _2^n)$.\\ How do we get this number? Let's do the math again.\\ In order to get the depth of the heap, we need to find out how many levels the heap has. The root element which needed to be bubbled down will move through the following steps;\\ $1,2,4,8,16, \dots  ,n$\\ Which can be rewritten as $2^0,2^1,2^2,2^3,2^4, \dots  ,2^k$\\ k is the depth of the heap, and $k = \log  _2^n$.\\}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Reading Note on CLRS Chapter 6.3}{27}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Heapify is the process of reshaping a binary tree into a heap data structure.\\ The main idea of min heapify is to make sure that the root of the tree is the smallest element of the entire tree.\\ The min heapify procedure is a recursive one.\\}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Priority Queue, Heapify and Heapsort}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.1}Inserting into a heap}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{First we need to understand how to insert an element into a heap.\\ The main idea is to insert the element at the end of the array and then bubble up the element until it is in the right position.\\ That is to say, insertion = insert to the end + bubble up.\\ The time cost of inserting to the end of the heap is just as simple as $\theta (1)$.\\ Then we need to bubble up the element. The time cost is $\theta (\log  _2^n)$.\\ So the total time cost is just $\theta (\log  _2^n)$.\\}{28}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.2}Deleting from a heap}{29}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Bubble up for a fix.}}{29}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Then we come to the concept of deletion of a heap element.\\ 1. Replace the element that we want to delete with the last element of the heap.\\ 2. Adjust the lenth of the heap to n-1, assuming the original length is n. Now we can simply delete the last element.\\ 3. Bubble up or bubble down the element that we moved. The specific operation we use depends on the situation. We need only one operation for any cases.\\}{29}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Bubble down for a fix.}}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Here is a question regarding the fix operation after deletion of a heap.\\}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The cost of time for deletion consists of two parts, 1 swap and then 2 bubble up/down.\\ The time of cost is $\theta (1)$ for step 1 and $\theta (\log  _2^n)$ for step 2.\\ The total time cost is $\theta (\log  _2^n)$.\\}{30}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.3}Finding the smallest element of a heap}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The time cost of finding the smallest element of a min heap is $\theta (1)$.\\ However, finding the largest element of a min heap is pretty hard and inefficient.\\}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Now we are talking about priority queue.\\ Priority queue is a data structure that will allow us to insert elements and delete the smallest one.\\ The main idea of priority queue is to keep the smallest element at the root of the heap.\\ Priority queue has a priority setting.\\}{31}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{We can bubble down elements that are in the wrong position and then get a min heap.\\ This is the main idea of heapify.\\ The running time of heapify is $O(n\log  _2^n)$.\\}{32}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The overall running time of heapify is linear to the size of the array. That is to say, the running time is $\theta (n)$.\\}{33}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.5.4}Heapsort}{34}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Quiz: Bubble-Up/Down, Insertion and Deletion}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.1}Question 1}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.2}Question 2}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Quiz: Heapify, Priority Queue and Heapsort}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.1}Question 1}{38}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.2}Question 2}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Hashtables}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The basic idea of hashtables is to implement a mapping from keys to values.\\ What's important of a hashtable is not key or value, but a special function called hash function.\\ With the hash function implemented, the key will be converted to a hash value.\\ The hash value will be used as an index to store the value. That is to say, the key will be converted to an index. However, since the hash value is based on the key itself, different keys may have the same hashed values, which means the same index may be mapped with several keys. How do we deal with this?\\}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Chaining for Collision Resolution}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The chaining of a hashtable is to store all the values that have the same index in a linked list. This list is empty initially.\\ When we insert a new key-value pair, we will first calculate the hash value of the key. Then we will check if the index is empty. If it is, we will simply insert the key-value pair. If not, we will insert the new pair to the linked list.\\}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{If we want to delete, we will first calculate the hash value of the key. Then we will check if the index is empty. If it is, we will simply delete the key-value pair. If not, we will delete the key-value pair from the linked list.\\}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Load Factor}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The load factor is the ratio of the number of key-value pairs to the number of indices. Suppose we have $m$ slots(indices) and $n$ key-value pairs. The load factor is $n/m$.\\}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Rehashing}{41}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Rehashing is the process of creating a new hashtable with a larger number of slots. Since we need to go through all the key-value pairs (n pairs) and all the slots/indices (m slots) in the original hashtable in order to create the new one, The running time of rehashing is $O(n+m)$.\\}{42}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Quiz: Hashtables}{42}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Question 1}{42}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}Question 2}{43}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Introduction to Randomization, Average Case Complexity Analysis and Recurrences}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The topic of this section is Randomization in Algorithm and Data Structures.\\}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Algorithm that Use Randomness}{44}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The main idea of randomization is to use randomness in the algorithm.\\ The main advantage of randomization is that it can reduce the running time of the algorithm, so that we will focus on the average case of the running time rather than the worst case.\\ When looking at the running time of an algorithm, we typically consider the worst case scenario.\\ While in the case of randomization, the probability of worst case is relatively low because Of the nature of randomness. Therefore, it is not efficient to always consider the worst case, since it may not happen, and the average case of running time may be much lower and more realistic.\\}{45}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces When running into case 1, a.k.a. $flip(p) == True$, the running time will be $p*(1+T(p))$, and in the other case, when $flip(p) == False$, the running time will be $1*(1-p)$.}}{45}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Let's take a look at this $geometric(p)$ function.\\ We will introduce a new concept $T(p)$, which is the expected value of the geometric function, a unknown average case running time.\\}{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{ The unknown running time $T(p) = p*(1+T(p)) + (1-p)*1$, which can be simplified to $T(p) = 1/(1-p)$.\\ }{46}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Analysis of Algorithms: Recurrences}{47}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The $T(n)$ equation is a recursive function in this Mergesort function. If $n<=1$, the function returns 1, where running time is 1. Otherwise, the function will be $2*T(n/2) + n$ based on the function details.\\}}{47}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Here is a general form of a recursive function. \\ $T(n) = a*T(n/b) + \theta (n), if n > m$\\ $T(n) = Constant, if n <= m$}}{47}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Here is a general understanding of recurrences.\\}{48}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces We can assign $C(n) == \theta (n)$ at first.\\ Then, we assign $T(n)$ to the original function with an expanded form.\\ For example, by multiplying $2$, $T(n) = 2*T(n/2) + C(n) + D$ will be converted to $T(n) = 2(2T(n/4) + C(n/2) + D) + C(n) +D$.\\ This expansion the function will keep going on by 4, 8 ,16 etc., until it reaches the base case, as displayed in the firgure.\\ The running time of this function is $\theta (n^{\log  _b^a})$.\\}}{48}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{If we want to solve for the recurrence relations, we can use the expansion method.\\ The main idea is to expand the function until we reach the base case.\\}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Partition and Quicksort Algorithm}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Basic Idea and Pseudocode}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Quicksort is an efficient in-place sorting algorithm that uses the divide and conquer strategy.\\ Basically it divide the array into two parts, one with the elements smaller than the pivot and the other bigger.\\ At first glance, quicksort sounds a lot like mergesort, but the main difference is that in mergesort, the major time costing part is at the end of the function when merging is taking place; while in quicksort, we will conduct a lot of work by partitioning the array first.\\}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What is the partition function?\\ First we are going to choose a pivot element from within the array and it doesn't matter which one we choose. Then we will compare all the rest of the elements with the pivot and divide them into smaller-than-pivot and larger-than-pivot part. At this stage, we are not necessarily sorting what is being compared.\\ Now we get a updated array where the pivot is rested somewhere in the middle range of the array, and it is the final position for the pivot in the fully sorted array later. All we need to do now is to sort the left part and the right part.\\}{49}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Here is the pesudocode for quicksort. $left$ means the index of the first element in array $A$ and $right$ means the index of the last.\\}{50}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Quiz on Quicksort}{51}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Quiz 1\\}}{51}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Quiz 2\\}}{52}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Designing Partition Scheme and Correctness}{52}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{First we will go through the Lomuto Partition Scheme.\\ We will introduce two intermediate variables, $i$ and $j$, which are the index of the first element and the last element of the array.\\ Then we will choose the last element as the pivot.\\ The original array can be divided into four parts, $A[1, \dots  ,i-1], A[i, \dots  ,j-1], A[j, \dots  ,n-1], A[n]$.\\ The end of the first region can be either $i$ or $i-1$ as noted, which is not a big deal.\\ The main idea of the Lomuto Partition Scheme is to compare the element at index $j$ with the pivot.\\ If the element is smaller than the pivot, we will swap it with the element at index $i$ and then increase $i$ by 1.\\ That is to say, everything after $j$ is categorized as ``unprocessed''.}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Here is a step-by-step analysis of the partition scheme.\\ Let $i=0, j=1$, and the pivot is $x = A[n]$.\\ Under this situation, everything besides the pivot is unprocessed, located in Region 3.\\ $\forall Region1 < x$, $\forall Region2 >= x$,\\ if $A[j] < x$, we will swap $A[i]$ and $A[j]$, and then increase $i$ as well as $j$ by 1.\\ If $A[j] >= x$, we will do nothing but increasing $j$ by 1.\\ Here is the pseudocode for the Lomuto Partition Scheme.\\}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The running time of Lomuto Partition Scheme is just $n$.\\}{54}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Lomutio Partition Algorithm Quiz}{54}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Quiz 1\\}}{54}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Quiz 2\\}}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Analysis of Quicksort Alogorithm}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The best case analysis of quicksort is that what we call a balanced partition.\\ Although it rarely happens, the running time when occuring is $\theta (nlog^n)$.}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Now Let's take a look at the worst case analysis of quicksort algorithm.\\ The worst case of quicksort is when the pivot is the smallest or the largest element of the array. Therefore, either side of the pivot will be empty.\\ The running time of the worst case is $\theta (n^2)$, which equals to the running time of insertion sort.\\ So what we should do is to avoid the worst case by choosing a pivot randomly.\\ The expected running time of quicksort is $\theta (nlog^n)$.\\}{56}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Let's assume $T(n)$ is the expected (or average) running time of quicksort.\\ Here is the equation of $T(n)$.\\ $T(n) = 1/n [\sum _{i=1}^{n-1} T(i) + \sum _{i=1}^{n-1} T(n-i)] + n$\\ And it equals to\\ $T(n) = 2/n [\sum _{i=1}^{n-1} T(i)] + n$\\ The above is the compressed form of the equation. We can expand it like the following;\\ $T(n) = 2/n [T(1)+T(2)+ \cdots  + T(n-1)] + n$.\\ If we multiply $n$ on both sides, we get\\ $n*T(n) = 2[T(1)+T(2)+ \cdots  + T(n-1)] + n^2$.\\ If we replace $n$ with $n-1$, we will get\\ $(n-1)*T(n-1) = 2[T(1)+T(2)+ \cdots  + T(n-2)] + (n-1)^2$.\\ Then we can subtract $nT(n-1)$ from $nT(n)$ and get\\ $nT(n) - (n-1)T(n-1) = 2T(n-1) + 2n - n^2 + n - 1$.\\ $T(n) = T(n-1) + 1/nT(n-1) + 2 - 1/n$.\\ Now once again, we will try to expand the above equation.\\ For convenience, we will assume $2-1/n = \theta (1)$.\\ Then we will get $T(n) = T(n-1) + \theta (1)$\\ $T(n) = T(n-1) + \theta (1)$\\ $T(n) = T(n-2) + 2\theta (1)$\\ $\cdots  $ $T(n) = (n+1)/(n-j+1)T(n-j) + \sum _{i=1}^{j-1} (n+1)/(n-i)*2\theta (1) + \theta (1)$.\\ When $j == n$, the equation will be\\ $T(n) = \sum _{i=1}^{j-1} (n+1)/(n-i)*2\theta (1) + \theta (1)$.\\ $T(n) = [(n+1)/n + (n+1)/(n-1) + \cdots  + (n+1)/1]*2\theta (1) + \theta (1)$\\ $T(n) = 2\theta (1)[n+1][1 + 1/2 + 1/3 + \cdots  + 1/n] + \theta (1)$\\ The above equation is the harmonic series, in which the $1 + 1/2 + 1/3 + \cdots  + 1/n$ part is called the nth harmoic number, and it equals to $\theta (\log  ^n)$.\\ Therefore, $T(n) = 2\theta (1)\theta (\log  ^n)$,\\ Then we can get the final result of $T(n) = nlog^n$, a.k.a $\theta (n\log  ^n)$. Now we can say, if we choose a pivot randomly in Quicksort, the average running time will be $\theta (n\log  ^n)$.\\}{57}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Quiz on Quicksort Analysis}{58}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Quiz 1\\}}{58}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Quiz 2\\}}{59}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Quiz 3\\}}{60}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Quickselect Algorithm}{61}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The next step will be to perform a $quickselect(12-5,A>=p)$ operation, in order to find the $7th$ smallest element on the right side of the pivot, since the $12th$ smallest element is already larger than the 4 elements in the left side of the pivot.\\}}{61}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The main idea of Quickselect is to find the kth smallest element in an array.\\ We can use Quicksort to select better than the brute force method, which is just trying to sort the array first and then find the target by comparing every element.\\ The running time of the brute force method is $\theta (n\log  ^n)$, while the running time of Quickselect is $\theta (n)$.\\ Here is the logic of using quicksort for quckselect function.\\ At first, we will choose a pivot and partition the array.\\ Then we will compare the pivot with the target. Let's say the pivot is j in array A with n elements in total.\\ If $j == k$, we will return the pivot.\\ If $j > k$, we will recursively call quickselect on the left part of the pivot, a.k.a. $quickselect(A[1], \dots  ,A[j-1])$.\\ If $j < k$, we will recursively call quickselect on the right part of the pivot, a.k.a. $quickselect(A[j+1], \dots  ,A[n])$.\\}{62}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The running time of $T(n)$ is actually $\theta (n)$.\\}}{62}{}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Now let's go through a real example of quickselect.\\ Let's say we have an array $A = [3,2,1,5,4,6,7,8,9]$.\\ We want to find the 4th smallest element in the array.\\ Let's say we choose the pivot randomly and it's 5.\\ After partitioning, the array will be $A = [3,2,1,4,5,6,7,8,9]$.\\ The pivot is 5, which is the 5th smallest element. Since pivot > k, we need to focus on the left side of the pivot, which is $[3,2,1,4]$.\\ Now we need to find the 4th smallest element in the left side of the pivot.\\ Let's say we choose the pivot randomly and it's 2.\\ After partitioning, the array will be $A = [1,2,3,4]$.\\ The pivot is 2, which is the 2nd smallest element. Since pivot < k, we need to focus on the right side of the pivot, which is $[3,4]$.\\ Since the 4th smallest element is already larger than the 2 elements in the left side of the pivot, we can just find the $4-2=2th$ smallest elment in $[3,4]$, which is 4.\\}{63}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Quiz on Quickselect}{64}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Quiz 1\\}}{64}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Quiz 2\\}}{64}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Quiz 3\\}}{64}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Quiz 4\\}}{64}{}\protected@file@percent }
\gdef \@abspage@last{64}
